{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick implementation of a Term Document Matrix from fast.ai Machine Learning 1 [Lesson 10 (1:06:51)](https://youtu.be/37sFIak42Sc?t=4011). FastAI library can be downloaded from: https://github.com/fastai/fastai/tree/master/fastai\n",
    "\n",
    "***NOTE*** this is a toy implementation that only counters *whether* a word is in a line instead of counting *how many times* a word appears.\n",
    "\n",
    "-- Wayne Nixalo\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term-Document Matrix example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words(corpus):\n",
    "    bag = dict()\n",
    "    for text in corpus:\n",
    "        for word in text.split(' '):\n",
    "            bag[word] = 1\n",
    "    return list(bag.keys())\n",
    "\n",
    "def print_TD_matrix():\n",
    "    \n",
    "    bow = bag_of_words\n",
    "\n",
    "    # Print out formatted Term-Document Matrix\n",
    "    for i in range(len(corpus)+2):\n",
    "        if i == 0:\n",
    "            print(f'{\"TEXT\":^18} | LABEL | {\"FEATURES (f)\":^27}')\n",
    "        elif i == 1:\n",
    "            print(f'{\" \":^18} | {\" \":<5} | {bow[0]:5}{bow[1]:^5}{bow[2]:^4}{bow[3]:}{bow[4]:^7}{bow[5]:^4}')\n",
    "        else:\n",
    "            print(f'{corpus[i-2]:<18} | {labels[i-2]:^6}| '\n",
    "                  f'{TD_matrix[i-2][0]:^3}{TD_matrix[i-2][1]:^7}{TD_matrix[i-2][2]:^3}'\n",
    "                  f'{TD_matrix[i-2][3]:^7}{TD_matrix[i-2][4]:^4}{TD_matrix[i-2][4]:^5}')\n",
    "        if i == len(corpus)+1:\n",
    "            print(f'{\"-\"*26:<26} | '\n",
    "                  f'{\"1\":^3}{\"1\":^7}{\"1\":^3}'\n",
    "                  f'{\"1\":^7}{\"1\":^4}{\"1\":^5}')\n",
    "            for f in range(len(frequencies)):\n",
    "                print(f'{\"p(c=\"+str(1-f)+\")\":<18} | {probabilities[1-f]:^5} | '\n",
    "                      f'{frequencies[f][0]:^3.2f}{frequencies[f][1]:^6.2f}{frequencies[f][2]:^5.2f}'\n",
    "                      f'{frequencies[f][3]:^4.2f}{frequencies[f][4]:^6.2f}{frequencies[f][5]:^2.2f}')\n",
    "\n",
    "def print_NB():\n",
    "    p_doc = [[1 for i in range(n_docs)] for p in range(len(probabilities.keys()))]\n",
    "    for d in range(n_docs):\n",
    "        λ = labels[d]\n",
    "        for w in range(n_wrds):\n",
    "            p_doc[TD_matrix[d][w]][d] *= frequencies[labels[λ]][w]\n",
    "            \n",
    "    print(f'{\"p(d|1)\":>13}{\"p(d|0)\":>7}{\"ratio\":>6}')\n",
    "    for d in range(n_docs):\n",
    "        print(f'Doc {d+1:}: {p_doc[0][d]:^6.3f}{p_doc[1][d]:^7.3f}{p_doc[0][d]/p_doc[1][d]:^7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       TEXT        | LABEL |        FEATURES (f)        \n",
      "                   |       | this movie is good  the  bad \n",
      "this movie is good |   1   |  1    1    1    1    0    0  \n",
      "the movie is good  |   1   |  0    1    1    1    1    1  \n",
      "this movie is bad  |   0   |  1    1    1    0    0    0  \n",
      "the movie is bad   |   0   |  0    1    1    0    1    1  \n",
      "-------------------------- |  1    1    1    1    1    1  \n",
      "p(c=1)             |  0.5  | 0.67 1.00 1.00 1.00 0.67 0.33\n",
      "p(c=0)             |  0.5  | 0.67 1.00 1.00 0.33 0.67 1.00\n",
      "\n",
      "       p(d|1) p(d|0) ratio\n",
      "Doc 1: 0.667  0.222  3.000 \n",
      "Doc 2: 0.667  0.222  3.000 \n",
      "Doc 3: 0.222  0.667  0.333 \n",
      "Doc 4: 0.222  0.667  0.333 \n"
     ]
    }
   ],
   "source": [
    "corpus = ['this movie is good', 'the movie is good', \n",
    "        'this movie is bad', 'the movie is bad']\n",
    "bag_of_words = unique_words(corpus)\n",
    "\n",
    "n_docs = len(corpus)\n",
    "n_wrds = len(bag_of_words)\n",
    "\n",
    "labels = [1,1,0,0]\n",
    "labels_avg = sum(labels)/n_docs\n",
    "probabilities = {0:1-labels_avg, 1:labels_avg}\n",
    "\n",
    "TD_matrix = [[int(word in text) for word in bag_of_words] for text in corpus]\n",
    "frequencies = [[(sum([TD_matrix[r+(f*2)][c] \n",
    "                      for r in range(n_docs//2)]) + 1)/(n_docs//2 + 1) \n",
    "                for c in range(n_wrds)] for f in range(2)]\n",
    "\n",
    "\n",
    "# Display Bag of Words representation of reviews\n",
    "print_TD_matrix()\n",
    "print()\n",
    "print_NB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn has a function to create a Term-Document matrix:\n",
    "\n",
    "`from sklearn.feature_extraction.text import CountVectorizer`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing and Term-Document Matrix Creation\n",
    "\n",
    "Using IMDB movie reviews dataset wrt Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset can be downloaded from terminal via:\n",
    "\n",
    "```\n",
    "wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "\n",
    "gunzip aclImdb_v1.tar.gz\n",
    "\n",
    "tar -xvf aclImdb_v1.tar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.nlp import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = pathlib.Path('../../data/aclImdb')\n",
    "names = ['neg', 'pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, trn_y = texts_labels_from_folders(PATH / 'train', names)\n",
    "val, val_y = texts_labels_from_folders(PATH / 'test', names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pathlib`: [docs](https://docs.python.org/3/library/pathlib.html) | [cheatsheet](http://pbpython.com/pathlib-intro.html)\n",
    "\n",
    "```\n",
    "tokenize: <function fastai.text.tokenize>\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "```\n",
    "\n",
    "\n",
    "[`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) converts a collection of text documents to a matrix of token counts (part of `sklearn.feature_extraction.text`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=tokenize) # the 'model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit_transform(trn)` finds the vocabulary in the training set. It also tranforms the training set into a Term-Document Matrix. Since we have to apply the *same transformation* to our validation set, the second line uses just the method `transform(val)`.\n",
    "\n",
    "`trn_term_doc` and `val_term_doc` are sparse matrices. `trn_term_doc[i]` represents training document `i` and it contains a count of words for each document for each word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_term_doc = vectorizer.fit_transform(trn) # create TD mat & transform\n",
    "val_term_doc = vectorizer.transform(val)     # apply same transformation (vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x75132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3749745 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc  # 75,132 unique words in vocab for 25,000 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x75132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 104 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc[0]  # 104 unique words in this review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample of words in vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cabbage', 'cabbages', 'cabbie', 'cabby', 'cabel']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names(); n = 10000; vocab[n:n+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking up vocab index of a word\n",
    "vectorizer.vocabulary_['cabbage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking TDF of term 10000 in doc 0\n",
    "trn_term_doc[0,10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **log-count ratio** $r$ for each word $f$ is defined as:\n",
    "\n",
    "$r = \\log \\frac{\\text{ratio of feature }\\ f \\ \\text{ in positive documents}}{\\text{ratio of feature }\\ f \\ \\text{ in negative documents}}$\n",
    "\n",
    "ratio of feature $f$ in positive documents is the **number of times a positive document has a feature $\\boldsymbol{f}$** divided by the **number of positive documents**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Latex reference](https://www.sharelatex.com/learn/Fractions_and_Binomials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the Log allows us to add & subtract things instead of multiplying and dividing. It also aids in numerical fidelity: after numerous multiplications near zero, products can become so small as to exceed machine precision limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trn_term_doc # indep var\n",
    "y = trn_y        # dep var\n",
    "\n",
    "p = x[y==1].sum(0) + 1  # sum positives\n",
    "q = x[y==0].sum(0) + 1  # sum negatives\n",
    "r = np.log((p/p.sum())/(q/q.sum())) # log of ratios\n",
    "b = np.log(len(p)/len(q)) # ratio of class probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula for Naïve Bayes:\n",
    "\n",
    "* for each document: multiply the Bayes probabilities by the counts : Matrix Multiply\n",
    "\n",
    "* add log of class ratios : add `b`\n",
    "\n",
    "* compare to zero (not 1 since we're in Logspace now)\n",
    "\n",
    "* compare to mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8074"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_preds = val_term_doc @ r.T + b\n",
    "preds = pre_preds.T > 0\n",
    "(preds == val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80.7% accuracy.\n",
    "\n",
    "Binarized Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82624"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_preds = val_term_doc.sign() @ r.T + b\n",
    "preds = pre_preds.T > 0\n",
    "(preds == val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of assuming the coefficients in $r$ are what should be used, we can learn them instead. Here's how we can fit logistic regression where the features are the unigrams.\n",
    "\n",
    "This'll give us something with the same functional form as before (`val_term_doc @ r.T + b`), but instead of using a theoretical fixed $r$ and $b$, they'll be calculated based on logistic regression.\n",
    "\n",
    "`dual=True` makes LogisticRegression run faster when the matrix is wider than it is tall by using a mathematically equivalent reformulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85728"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(C=1e8, dual=True)\n",
    "model.fit(x, y)\n",
    "preds = model.predict(val_term_doc)\n",
    "(preds == val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binarized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85504"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(C=1e8, dual=True)\n",
    "model.fit(trn_term_doc.sign(), y)\n",
    "preds = model.predict(val_term_doc.sign())\n",
    "(preds == val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogReg is going to give us a coefficient for every term in our vocabulary. This gives us about 75,000 coefficients, which seems high given we have only about 25,000 reviews.\n",
    "\n",
    "We can regularize this by using sklearn's built in regularizer via the `C` parameter. Smaller `C` means more regularization (so large `C=1e8` effectively shuts it off)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88236"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(C=0.1, dual=True)\n",
    "model.fit(x, y)\n",
    "preds = model.predict(val_term_doc)\n",
    "(preds == val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88404"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(C=0.1, dual=True)\n",
    "model.fit(x.sign(), y)\n",
    "preds = model.predict(val_term_doc.sign())\n",
    "(preds == val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* L1 Reg looks at absolute value of weights\n",
    "* L2 Reg looks at the weights squared\n",
    "\n",
    "L1 Reg will try to make as many things Zero as possible. L2 Reg tries to make *everything* smaller.\n",
    "\n",
    "For our purposes here, L1 Regularization is appropriate - though L2 is the default w/ sklearn LogisticRegression (and the only way to use `dual=True`).\n",
    "\n",
    "The difference between L1/L2 isn't so important in modern ML since we rarely try to directly interpret the coefficients; instead we try to understand our models through interrogation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram with NB features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next model is a verison of Logistic Regression with Naïve Bayes features described [here](https://www.aclweb.org/anthology/P12-2018). For every document we compute binarized features as described above, but this time we use bigrams and trigrams too. Each feature is a Log-count ratio. A Logistic Regression model is then trained to predict sentiment.\n",
    "\n",
    "When you initialize the `CountVectorizer`, you can also ask for n-grams -- by default we get unigrams (single words). If we specify `ngram_range=(1,3)` we'll get bigrams and trigrams in addition to unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,3), \n",
    "                             tokenizer=tokenize, max_features=800000)\n",
    "trn_term_doc = vectorizer.fit_transform(trn)\n",
    "val_term_doc = vectorizer.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 800000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simplicity and',\n",
       " 'simplicity of',\n",
       " 'simplicity of the',\n",
       " 'simplicity that',\n",
       " 'simplification']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 600000; vocab[n:n+5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram features dramatically improve Naïve Bayes and Logistic Regression. This technique is very helpful for taking advantage of Bag of Words approaches because it allows us to see the difference between [not good] vs [not bad], or [good] vs [\" good \"] and etc.\n",
    "\n",
    "The `CountVectorizer` will sort n-grams by how often the appear and cutoff at the `max_features` most common n-grams. `max_features` can be specified as some 'large' number that loads reasonably quickly and provides good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr(y_i):\n",
    "    p = x[y == y_i].sum(0)\n",
    "    return (p + 1)/((y == y_i).sum() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = trn_y\n",
    "x = trn_term_doc.sign()\n",
    "val_x = val_term_doc.sign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.log(pr(1) / pr(0)) # <==> r = np.log((p/p.sum())/(q/q.sum()))\n",
    "b = np.log((y==1).mean() / (y==0).mean()) # <==> b = np.log(len(p)/len(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.905"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(C=0.1, dual=True)\n",
    "model.fit(x, y)\n",
    "\n",
    "preds = model.predict(val_x)\n",
    "(preds.T == val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-count ratio r:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 800000),\n",
       " matrix([[-0.05468, -0.161  , -0.24784, ...,  1.09861, -0.69315, -0.69315]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.94678, 0.85129, 0.78049, ..., 3.     , 0.5    , 0.5    ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting regularized Logistic Regression where features are the trigrams' Log-count ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91768"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_nb = x.multiply(r)\n",
    "model = LogisticRegression(dual=True, C=0.1)\n",
    "model.fit(x_nb, y)\n",
    "\n",
    "val_x_nb = val_x.multiply(r)\n",
    "preds = model.predict(val_x_nb)\n",
    "(preds.T == val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag_of_words to string for printing\n",
    "# bag = ''\n",
    "# for word in bag_of_words:\n",
    "#     bag += word + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilities = {0:0.5, 1:0.5}\n",
    "# bow_dict = {word:i for i, word in enumerate(bag_of_words)}\n",
    "\n",
    "# frequency = [[0 for i in range(len(bag_of_words))] for i in range(len(probabilities.keys()))]\n",
    "# for i,text in enumerate(corpus):\n",
    "#     for word in text.split(' '):\n",
    "#         frequency[labels[i]][bow_dict[word]] += 1\n",
    "# frequency = [list(map(lambda x : x /(1 + len(TD_matrix)//2), frequency[i])) for i in range(len(frequency))]\n",
    "\n",
    "\n",
    "#                   f'{(sum([TD_matrix[i][0] for i in range(len(TD_matrix))]) + 1)/(len(TD_matrix)+1):^3}'\n",
    "#                   f'{(sum([TD_matrix[i][1] for i in range(len(TD_matrix))]) + 1)/(len(TD_matrix)+1):^7}'\n",
    "#                   f'{(sum([TD_matrix[i][2] for i in range(len(TD_matrix))]) + 1)/(len(TD_matrix)+1):^3}'\n",
    "#                   f'{(sum([TD_matrix[i][3] for i in range(len(TD_matrix))]) + 1)/(len(TD_matrix)+1):^5}'\n",
    "#                   f'{(sum([TD_matrix[i][4] for i in range(len(TD_matrix))]) + 1)/(len(TD_matrix)+1):^5}'\n",
    "#                   f'{(sum([TD_matrix[i][5] for i in range(len(TD_matrix))]) + 1)/(len(TD_matrix)+1):^3}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for-loop way\n",
    "# frequencies = [[],[]]\n",
    "# for f in range(len(frequencies)):\n",
    "#     for c in range(len(TD_matrix[0])):\n",
    "#         tot = 0\n",
    "#         for r in range(len(TD_matrix)//2):\n",
    "#             tot += TD_matrix[r+(f*2)][c]\n",
    "#         frequencies[f].append((tot + 1)/(len(TD_matrix)//2 + 1))       \n",
    "\n",
    "# list-comprehension way\n",
    "# frequencies = [[(sum([TD_matrix[r+(f*2)][c] for r in range(len(TD_matrix)//2)]) + 1)/(len(TD_matrix)//2 + 1) for c in range(len(TD_matrix[0]))] for f in range(2)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
